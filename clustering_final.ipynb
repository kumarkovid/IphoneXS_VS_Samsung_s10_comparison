{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.cluster import KMeansClusterer,cosine_distance\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import json\n",
    "from numpy.random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K means based on Cosine distance\n",
    "\n",
    "def cluster_kmean(data):\n",
    "    data=pd.read_csv(data, header=0, names=['commentText', 'label'])\n",
    "\n",
    "    # generating the tfidf matrix\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english',\\\n",
    "                             min_df=5)\n",
    "    #splitting the data into training and testing set.\n",
    "    \n",
    "    train=data[:800]\n",
    "    test=data[800:999]\n",
    "\n",
    "    dtm=tfidf_vect.fit_transform(train[\"commentText\"])\n",
    "    \n",
    "    #number of cluster that we want is 2\n",
    "    num_clusters=2\n",
    "    clusterer = KMeansClusterer(num_clusters, cosine_distance, repeats=1)\n",
    "    clusters = clusterer.cluster(dtm.toarray(),assign_clusters=True)\n",
    "    \n",
    "    #working on the test data\n",
    "    test_dtm = tfidf_vect.transform(test[\"commentText\"])\n",
    "    \n",
    "    predicted = [clusterer.classify(v) for v in test_dtm.toarray()]\n",
    "  \n",
    "    #creating the confusion matrix\n",
    "    confusion_df = pd.DataFrame(list(zip(test['label'].values, predicted)),\\\n",
    "                           columns = [\"label\", \"cluster\"])\n",
    "    \n",
    "    #creating a cross tab\n",
    "    x=pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)\n",
    "   \n",
    "    #selecting 2 labels\n",
    "    cluster_dict = {0:0, 1:1}\n",
    "    predicted_target=[cluster_dict[i] for i in predicted]\n",
    "    \n",
    "    #printing the metrics\n",
    "    print(metrics.classification_report(test[\"label\"], predicted_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_cluster_kmean(data): #euclidean distance with sklearn, first approach was nltk and cosine distance\n",
    "    \n",
    "    data=pd.read_csv(data, header=0, names=['commentText', 'label'])\n",
    "    #train, test = train_test_split(data, test_size=0.2, random_state=0)\n",
    "    #data=data.dropna(subset = ['commentText', 'label'])\n",
    "\n",
    "    #Splitting the data into training and testing\n",
    "    train=data[:800]\n",
    "    test=data[800:999]\n",
    "    \n",
    "    #generating the tfidf matrix \n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=5) \n",
    "    dtm= tfidf_vect.fit_transform(train[\"commentText\"])\n",
    "    \n",
    "    #setting the number of clusters to 2\n",
    "    num_clusters=2\n",
    "    \n",
    "    km = KMeans(n_clusters=num_clusters, n_init=20).fit(dtm)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    test_dtm = tfidf_vect.transform(test['commentText'])\n",
    "    \n",
    "    #predicting the labels of the test data \n",
    "    predicted = km.predict(test_dtm)\n",
    "    \n",
    "    #generating a confusion matrix\n",
    "    confusion_df = pd.DataFrame(list(zip(test['label'], predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "    \n",
    "    #creating a cross tab\n",
    "    x=pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)\n",
    "    cluster_dict={0:0, 1:1}\n",
    "      \n",
    "    predicted_target=[cluster_dict[i] for i in predicted]\n",
    "\n",
    "    #print(test[:100])\n",
    "    #printing the metrics\n",
    "    print(metrics.classification_report(test[\"label\"], predicted_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.19      0.88      0.32        40\n",
      "         1.0       0.71      0.08      0.14       159\n",
      "\n",
      "   micro avg       0.24      0.24      0.24       199\n",
      "   macro avg       0.45      0.48      0.23       199\n",
      "weighted avg       0.60      0.24      0.17       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #cluster_kmean(\"/Users/divyaj_podar/Desktop/Final Project/s10.csv\")\n",
    "    #cluster_kmean(\"/Users/divyaj_podar/Desktop/Final Project/iphone.csv\")\n",
    "\n",
    "    alt_cluster_kmean(\"/Users/divyaj_podar/Desktop/Final Project/s10.csv\")\n",
    "    #alt_cluster_kmean(\"/Users/divyaj_podar/Desktop/Final Project/iphone.csv\")\n",
    "\n",
    "    #first method working alot better for iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
